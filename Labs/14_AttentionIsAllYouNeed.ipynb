{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Networks and Deep Learning\n",
        "\n",
        "---\n",
        "\n",
        "## Lecture 8a: Attention Is All You Need\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=16IF6WfD_-vJ25FHgfhEmTvzI6QGctzhQ\" width=\"500\"/>\n",
        "\n"
      ],
      "metadata": {
        "id": "Gj4JkdUfKlqD"
      },
      "id": "Gj4JkdUfKlqD"
    },
    {
      "cell_type": "markdown",
      "id": "9ed846a5",
      "metadata": {
        "id": "9ed846a5"
      },
      "source": [
        "## ‚öôÔ∏è Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7eb49c12",
      "metadata": {
        "id": "7eb49c12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b3cddb6-5d90-4c44-a0d2-cfed4870b414"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.1\n"
          ]
        }
      ],
      "source": [
        "# Set seed for reproducibility\n",
        "seed = 42\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Import TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "# Reduce TensorFlow verbosity\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# Print TensorFlow version\n",
        "print(tf.__version__)\n",
        "\n",
        "# Import other libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e6c9288",
      "metadata": {
        "id": "2e6c9288"
      },
      "source": [
        "## üõ†Ô∏è Scaled Dot-Product Attention\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1bHYwJ61EZQGebSYQqyzh5byKDDruGmls\" width=\"350\"/>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch size, sequence lengths, and model dimension\n",
        "batch_size = 2\n",
        "seq_len_q = 4  # Query sequence length\n",
        "seq_len_kv = 6  # Keys and values sequence length\n",
        "d_model = 8  # Model dimensionality\n",
        "\n",
        "# Generate random tensors for Q, K, and V\n",
        "Q = tf.random.normal((batch_size, seq_len_q, d_model))\n",
        "K = tf.random.normal((batch_size, seq_len_kv, d_model))\n",
        "V = tf.random.normal((batch_size, seq_len_kv, d_model))\n",
        "\n",
        "# Print shapes of Q, K, and V\n",
        "print(f\"Q shape: {Q.shape}\")\n",
        "print(f\"K shape: {K.shape}\")\n",
        "print(f\"V shape: {V.shape}\")"
      ],
      "metadata": {
        "id": "LZveImqUOqhD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ab8684-9e48-4e75-b6d4-9df3f3038b6c"
      },
      "id": "LZveImqUOqhD",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q shape: (2, 4, 8)\n",
            "K shape: (2, 6, 8)\n",
            "V shape: (2, 6, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaled Dot-Product Attention - Cross-Attention**\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1sD-Vx2-kwyU4Nq-wthWHkCfrMSyp-85g\" width=\"600\"/>\n"
      ],
      "metadata": {
        "id": "hz_lIztURi7l"
      },
      "id": "hz_lIztURi7l"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define cross-attention layer with scaling and dot-product score mode\n",
        "cross_attn = tf.keras.layers.Attention(use_scale=True, score_mode='dot')\n",
        "\n",
        "# Compute cross-attention output with Q as query, and K, V from another sequence\n",
        "cross_output = cross_attn([Q, V, K])\n",
        "\n",
        "# Print shape of the cross-attention output\n",
        "print(f\"Cross-Attention output shape: {cross_output.shape}\")"
      ],
      "metadata": {
        "id": "Aw-xk-OURiUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfde8cab-6e1d-4059-f293-2768aea028ba"
      },
      "id": "Aw-xk-OURiUP",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Attention output shape: (2, 4, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaled Dot-Product Attention - Self-Attention**"
      ],
      "metadata": {
        "id": "WeBkv-OHRTwV"
      },
      "id": "WeBkv-OHRTwV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define self-attention layer with scaling and dot-product score mode\n",
        "self_attn = tf.keras.layers.Attention(use_scale=True, score_mode='dot')\n",
        "\n",
        "# Compute self-attention output with Q used as query, keys, and values\n",
        "self_output = self_attn([Q, Q, Q])\n",
        "\n",
        "# Print shape of the self-attention output\n",
        "print(f\"Self-Attention output shape: {self_output.shape}\")"
      ],
      "metadata": {
        "id": "iqLqZENqRJOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cfa9235-f13b-478d-8c45-892830bd47c6"
      },
      "id": "iqLqZENqRJOT",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-Attention output shape: (2, 4, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fd44fa2",
      "metadata": {
        "id": "3fd44fa2"
      },
      "source": [
        "## üõ†Ô∏è Multi-Head Attention\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1SD0qnvFWCHRwop0TTWeQPzdLPKyFO2bD\" width=\"350\"/>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define number of attention heads and compute head dimension\n",
        "num_heads = 4\n",
        "head_dim = d_model // num_heads\n",
        "\n",
        "# Print computed head dimension\n",
        "print(f\"Head dimension: {head_dim}\")"
      ],
      "metadata": {
        "id": "juzt7AUXSyzl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f117bc8a-234f-485f-bc59-8f5858e9ad8a"
      },
      "id": "juzt7AUXSyzl",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head dimension: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaled Multi-Head Dot-Product Attention - Cross-Attention**\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=11R6PgMYOVs1bWsseDZhMq16mTBYaWkZ0\" width=\"800\"/>\n"
      ],
      "metadata": {
        "id": "6JEhIYoITUvJ"
      },
      "id": "6JEhIYoITUvJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a multi-head attention layer with 2 heads and specified key dimension\n",
        "mha = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=head_dim)\n",
        "\n",
        "# Compute multi-head attention output and attention scores\n",
        "output, attention_heads = mha(query=Q, value=V, key=K, return_attention_scores=True)\n",
        "\n",
        "# Print shapes of the multi-head attention output and attention weights\n",
        "print(f\"Multi-Head Attention output shape: {output.shape}\")\n",
        "print(f\"Multi-Head Attention weights shape: {attention_heads.shape}\")"
      ],
      "metadata": {
        "id": "X0o_47RPSyxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed3244d9-2fc3-4801-ad2a-99ec5c26c4b8"
      },
      "id": "X0o_47RPSyxJ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-Head Attention output shape: (2, 4, 8)\n",
            "Multi-Head Attention weights shape: (2, 2, 4, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaled Multi-Head Dot-Product Attention - Self-Attention**"
      ],
      "metadata": {
        "id": "le02zIR4TZUB"
      },
      "id": "le02zIR4TZUB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a multi-head attention layer with 2 heads and specified key dimension\n",
        "mha = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=head_dim)\n",
        "\n",
        "# Compute multi-head self-attention output and attention scores\n",
        "output, attention_heads = mha(query=Q, value=Q, key=Q, return_attention_scores=True)\n",
        "\n",
        "# Print shapes of the multi-head attention output and attention weights\n",
        "print(f\"Multi-Head Attention output shape: {output.shape}\")\n",
        "print(f\"Multi-Head Attention weights shape: {attention_heads.shape}\")"
      ],
      "metadata": {
        "id": "INDrEmpISyud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8603f583-af7d-4225-dffc-e7f1cf01df2f"
      },
      "id": "INDrEmpISyud",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-Head Attention output shape: (2, 4, 8)\n",
            "Multi-Head Attention weights shape: (2, 2, 4, 4)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}